{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All Required Imports \n",
    "import os\n",
    "import datetime\n",
    "now = datetime.datetime.now\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sbn # better plotting and aesthetics\n",
    "from pathlib import Path # just a utility for better cross-platform file-loading\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "import sklearn.neural_network as nn\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sbn  # this is a nice plotting library\n",
    "import warnings\n",
    "\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, BatchNormalization  # we have 2D images\n",
    "from tensorflow.keras.layers import Dense, MaxPool2D, LeakyReLU, ReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# size of pooling area for max pooling\n",
    "pool_size = 2\n",
    "# convolution kernel size\n",
    "kernel_size = 3\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Number of classes where training is done on all layers\n",
    "num_train_classes = 7\n",
    "\n",
    "# Number of classes where training is not done on all layers\n",
    "num_transfer_learn_classes = 3\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "#Reference\n",
    "#https://keras.io/examples/mnist_transfer_cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test):\n",
    "   \n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    train_start_time = now()\n",
    "    model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    verbose=1,  # show progress bar\n",
    ")\n",
    "    print('Training time: %s' % (now() - train_start_time))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)   #verbose=0 to avoid unwanted outputs\n",
    "    print('Test Loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41935 samples\n",
      "Epoch 1/10\n",
      "41935/41935 [==============================] - 81s 2ms/sample - loss: 0.1591 - accuracy: 0.9494\n",
      "Epoch 2/10\n",
      "41935/41935 [==============================] - 80s 2ms/sample - loss: 0.0505 - accuracy: 0.9862\n",
      "Epoch 3/10\n",
      "41935/41935 [==============================] - 84s 2ms/sample - loss: 0.0388 - accuracy: 0.9886\n",
      "Epoch 4/10\n",
      "41935/41935 [==============================] - 89s 2ms/sample - loss: 0.0299 - accuracy: 0.9917\n",
      "Epoch 5/10\n",
      "41935/41935 [==============================] - 83s 2ms/sample - loss: 0.0255 - accuracy: 0.9921\n",
      "Epoch 6/10\n",
      "41935/41935 [==============================] - 83s 2ms/sample - loss: 0.0215 - accuracy: 0.9932\n",
      "Epoch 7/10\n",
      "41935/41935 [==============================] - 90s 2ms/sample - loss: 0.0200 - accuracy: 0.9938\n",
      "Epoch 8/10\n",
      "41935/41935 [==============================] - 93s 2ms/sample - loss: 0.0190 - accuracy: 0.9943\n",
      "Epoch 9/10\n",
      "41935/41935 [==============================] - 87s 2ms/sample - loss: 0.0155 - accuracy: 0.9953\n",
      "Epoch 10/10\n",
      "41935/41935 [==============================] - 81s 2ms/sample - loss: 0.0157 - accuracy: 0.9952\n",
      "Training time: 0:14:12.421031\n",
      "Test Loss: 0.016816775628637613\n",
      "Test accuracy: 0.99456286\n",
      "Train on 18065 samples\n",
      "Epoch 1/10\n",
      "18065/18065 [==============================] - 11s 631us/sample - loss: 0.0457 - accuracy: 0.9846\n",
      "Epoch 2/10\n",
      "18065/18065 [==============================] - 11s 590us/sample - loss: 0.0167 - accuracy: 0.9945\n",
      "Epoch 3/10\n",
      "18065/18065 [==============================] - 11s 594us/sample - loss: 0.0114 - accuracy: 0.9963\n",
      "Epoch 4/10\n",
      "18065/18065 [==============================] - 10s 571us/sample - loss: 0.0077 - accuracy: 0.9977\n",
      "Epoch 5/10\n",
      "18065/18065 [==============================] - 10s 569us/sample - loss: 0.0087 - accuracy: 0.9968\n",
      "Epoch 6/10\n",
      "18065/18065 [==============================] - 10s 563us/sample - loss: 0.0062 - accuracy: 0.9977\n",
      "Epoch 7/10\n",
      "18065/18065 [==============================] - 10s 562us/sample - loss: 0.0056 - accuracy: 0.9985\n",
      "Epoch 8/10\n",
      "18065/18065 [==============================] - 10s 567us/sample - loss: 0.0069 - accuracy: 0.9978\n",
      "Epoch 9/10\n",
      "18065/18065 [==============================] - 10s 573us/sample - loss: 0.0060 - accuracy: 0.9977\n",
      "Epoch 10/10\n",
      "18065/18065 [==============================] - 10s 568us/sample - loss: 0.0056 - accuracy: 0.9978\n",
      "Training time: 0:01:44.644059\n",
      "Test Loss: 0.016776055597074967\n",
      "Test accuracy: 0.9956825\n"
     ]
    }
   ],
   "source": [
    "# Bring the input data between 0 and 1\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "no_of_samples, data_dim1, data_dim2 = x_train.shape\n",
    "#Reshape to have each sample data in grey scale Image format\n",
    "x_train = np.reshape(x_train, (no_of_samples, data_dim1, data_dim2, 1))\n",
    "x_train = x_train.astype('float32')\n",
    "\n",
    "no_of_samples, data_dim1, data_dim2 = x_test.shape\n",
    "#Reshape to have each sample data in grey scale Image format\n",
    "x_test = np.reshape(x_test, (no_of_samples, data_dim1, data_dim2, 1))\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "\n",
    "# create two datasets one with digits below 7 and one with 7 and above\n",
    "x_train_lt7 = x_train[y_train < 7]\n",
    "y_train_lt7 = y_train[y_train < 7]\n",
    "x_test_lt7 = x_test[y_test < 7]\n",
    "y_test_lt7 = y_test[y_test < 7]\n",
    "\n",
    "\n",
    "x_train_gte7 = x_train[y_train >= 7]\n",
    "y_train_gte7 = y_train[y_train >= 7] \n",
    "x_test_gte7 = x_test[y_test >= 7]\n",
    "y_test_gte7 = y_test[y_test >= 7] \n",
    "\n",
    "\n",
    "# define two groups of layers: feature (convolutions) and classification (dense)\n",
    "feature_layers = [\n",
    "    Conv2D(32, kernel_size,\n",
    "           padding='valid',\n",
    "           input_shape=(28,28,1)),\n",
    "    Activation('relu'),\n",
    "    Conv2D(32, kernel_size),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(128, kernel_size),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Flatten(),\n",
    "]\n",
    "\n",
    "classification_layers = [\n",
    "    Dense(64),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.25),\n",
    "    Dense(16),\n",
    "    Activation('relu'),\n",
    "    Dense(num_train_classes),\n",
    "    Activation('softmax')\n",
    "]\n",
    "\n",
    "\n",
    "# convert class vectors to one hot encoding using Panda\n",
    "s = pd.Series(y_train_lt7)\n",
    "y_train_lt7 = pd.get_dummies(s)\n",
    "y_train_lt7 = y_train_lt7.values\n",
    "\n",
    "s = pd.Series(y_test_lt7)\n",
    "y_test_lt7 = pd.get_dummies(s)\n",
    "y_test_lt7 = y_test_lt7.values\n",
    "\n",
    "\n",
    "s = pd.Series(y_train_gte7)\n",
    "y_train_gte7 = pd.get_dummies(s)\n",
    "y_train_gte7 = y_train_gte7.values\n",
    "\n",
    "s = pd.Series(y_test_gte7)\n",
    "y_test_gte7 = pd.get_dummies(s)\n",
    "y_test_gte7 = y_test_gte7.values\n",
    "\n",
    "# create complete model\n",
    "model = Sequential(feature_layers + classification_layers)\n",
    "\n",
    "\n",
    "# train the whole model for 7-digit classification [0..7]\n",
    "train_model(model, x_train_lt7, y_train_lt7, x_test_lt7, y_test_lt7)\n",
    "\n",
    "# freeze feature layers and rebuild model to have the learning transfered to the new model\n",
    "trans_learning_model = Sequential()\n",
    "\n",
    "for i in range(len(feature_layers)):\n",
    "    layer = model.layers[i]\n",
    "    layer.trainable = False\n",
    "    trans_learning_model.add(layer)\n",
    "    \n",
    "\n",
    "# Add Trainable Layers    \n",
    "trans_learning_model.add((Dense(units=128, activation=\"relu\")))\n",
    "trans_learning_model.add(Dropout(0.1))\n",
    "trans_learning_model.add(Dense(units=16, activation=\"relu\"))\n",
    "trans_learning_model.add(Dense(num_transfer_learn_classes, activation='softmax'))\n",
    "\n",
    "# train model for 7, 8 and 9\n",
    "train_model(trans_learning_model, x_train_gte7, y_train_gte7, x_test_gte7, y_test_gte7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
